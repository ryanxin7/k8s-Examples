#################################################################################################################
# Create a filesystem with settings with replication enabled for a production environment.
# A minimum of 3 OSDs on different nodes are required in this example.
# If one mds daemon per node is too restrictive, see the podAntiAffinity below.
#  kubectl create -f filesystem.yaml
#################################################################################################################

apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: myfs
  namespace: rook-ceph # namespace:cluster
spec:
  # 元数据池配置，必须使用复制策略
  metadataPool:
    replicated:
      size: 3
      requireSafeReplicaSize: true
    parameters:
      pg_num: "128"  # 手动设置 PG 数量
      pgp_num: "128"  # 通常与 pg_num 相同
      compression_mode: none
  dataPools:
    - name: replicated
      failureDomain: host
      replicated:
        size: 3
        # 设置数据池的副本数量
        # Disallow setting pool with replica 1, this could lead to data loss without recovery.
        # Make sure you're *ABSOLUTELY CERTAIN* that is what you want
        requireSafeReplicaSize: true
      parameters:
        pg_num: "512"
        pgp_num: "512"
        compression_mode: none
  preserveFilesystemOnDelete: true
  # 元数据服务器 (MDS) 配置
  metadataServer:
    # The number of active MDS instances
    # 活跃的元数据服务器数量，通常至少为 1
    activeCount: 2
    # Whether each active MDS instance will have an active standby with a warm metadata cache for faster failover.
    # If false, standbys will be available, but will not have a warm cache.
    # 启用活动-备用模式，确保高可用性
    activeStandby: true
    # 定义元数据服务器（MDS）Pod 的调度规则
    # The affinity rules to apply to the mds deployment
    placement:
      #  nodeAffinity:
      #    requiredDuringSchedulingIgnoredDuringExecution:
      #      nodeSelectorTerms:
      #      - matchExpressions:
      #        - key: role
      #          operator: In
      #          values:
      #          - mds-node
      #  topologySpreadConstraints:
      # 配置 Pod 的拓扑分布约束
      #  tolerations:
      #  - key: mds-node
      #    operator: Exists
      #  podAffinity:
      # 配置 Pod 亲和性规则
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
                - key: app
                  operator: In
                  values:
                    - rook-ceph-mds
            ## Add this if you want to allow mds daemons for different filesystems to run on one
            ## node. The value in "values" must match .metadata.name.
            #    - key: rook_file_system
            #          operator: In
            #          values:
            #            - myfs
            # topologyKey: kubernetes.io/hostname will place MDS across different hosts
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - rook-ceph-mds
              # preferredDuringSchedulingIgnoredDuringExecution:是一种软调度规则，指定 Kubernetes 优先将 Pod 调度到符合条件的节点上，但如果无法满足，也不会阻止调度。
              # topologyKey: */zone can be used to spread MDS across different AZ
              # Use <topologyKey: failure-domain.beta.kubernetes.io/zone> in k8s cluster if your cluster is v1.16 or lower
              # Use <topologyKey: topology.kubernetes.io/zone>  in k8s cluster is v1.17 or upper
              topologyKey: topology.kubernetes.io/zone
    # A key/value list of annotations
    # annotations:
    #  key: value
    # A key/value list of labels
    # labels:
    #  key: value

    # 用于设置 MDS Pod 的 CPU 和内存资源限制及请求
    resources:
    # The requests and limits set here, allow the filesystem MDS Pod(s) to use half of one CPU core and 1 gigabyte of memory
      limits:
        cpu: "500m"
        memory: "1024Mi"
      requests:
        cpu: "500m"
        memory: "1024Mi"

    # 优先级类
    priorityClassName: system-cluster-critical
    livenessProbe:
      disabled: false
    startupProbe:
      disabled: false
  
  # 为 Ceph 文件系统启用快照镜像、管理快照调度和保留策略。
  # mirroring:
    # 启用 CephFS 镜像功能。这将使得文件系统的快照可以在集群之间同步
    # enabled: true
    # list of Kubernetes Secrets containing the peer token
    # for more details see: https://docs.ceph.com/en/latest/dev/cephfs-mirroring/#bootstrap-peers
    # Add the secret name if it already exists else specify the empty list here.
    # peers:
      # 这里你可以列出包含对等集群的共享密钥的 Kubernetes Secret 名称，这个密钥用于与另一个 Ceph 集群进行通信和同步。
      #secretNames:
        #- secondary-cluster-peer
    # specify the schedule(s) on which snapshots should be taken
    # see the official syntax here https://docs.ceph.com/en/latest/cephfs/snap-schedule/#add-and-remove-schedules
    # snapshotSchedules:
    # 指定要进行快照的文件系统路径
    #   - path: /
    #     interval: 24h # daily snapshots #定义快照的时间间隔
        # The startTime should be mentioned in the format YYYY-MM-DDTHH:MM:SS
        # If startTime is not specified, then by default the start time is considered as midnight UTC.
        # see usage here https://docs.ceph.com/en/latest/cephfs/snap-schedule/#usage
        # startTime: 2022-07-15T11:55:00 #可选的字段，指定快照的开始时间
    # manage retention policies
    # see syntax duration here https://docs.ceph.com/en/latest/cephfs/snap-schedule/#add-and-remove-retention-policies
    # snapshotRetention: #定义快照的保留时间
    #   - path: /
    #     duration: "h 24"
